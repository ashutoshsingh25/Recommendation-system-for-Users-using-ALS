{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import pickle\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "from sklearn.metrics.pairwise import cosine_distances, cosine_similarity\n",
    "import re\n",
    "import os\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "'''\n",
    "# Loading and processing data\n",
    "'''\n",
    " \n",
    "raw_data = pd.read_csv('subcat_378.csv')\n",
    "#raw_data = raw_data.drop(raw_data.columns[1], axis=1)\n",
    "raw_data = raw_data.loc[:,['GLUSR','MAPPED_MCAT','PURCHASE_CNT']]\n",
    "#raw_data = raw_data.drop(raw_data.columns[1], axis=1)\n",
    "raw_data.columns = ['user', 'mcat', 'purchase_cnt']\n",
    "\n",
    "# Drop NaN columns\n",
    "data = raw_data.dropna()\n",
    "data = data.copy()\n",
    "\n",
    "# Create a numeric user_id and mcat_id column\n",
    "data['user'] = data['user'].astype(\"category\")\n",
    "data['mcat'] = data['mcat'].astype(\"category\")\n",
    "data['user_id'] = data['user'].cat.codes\n",
    "data['mcat_id'] = data['mcat'].cat.codes\n",
    " \n",
    " # Create a lookup frame so we can get the mcat names back in \n",
    " # readable form later.\n",
    "item_lookup = data[['mcat_id', 'mcat']].drop_duplicates()\n",
    "item_lookup['mcat_id'] = item_lookup.mcat_id.astype(str)\n",
    "\n",
    "user_lookup = data[['user_id', 'user']].drop_duplicates()\n",
    "user_lookup['user_id'] = item_lookup.mcat_id.astype(str)\n",
    " \n",
    "data = data.drop(['user', 'mcat'], axis=1)\n",
    " \n",
    " # Drop any rows that have 0 purchases\n",
    "#data = data.loc[data.purchase_cnt != 0]\n",
    " \n",
    " # Create lists of all users, mcats and their purchase counts\n",
    "users = list(np.sort(data.user_id.unique()))\n",
    "mcats = list(np.sort(data.mcat_id.unique()))\n",
    "purchases = list(data.purchase_cnt)\n",
    " \n",
    " # Get the rows and columns for our new matrix\n",
    "rows = data.user_id.astype(int)\n",
    "cols = data.mcat_id.astype(int)\n",
    " \n",
    " # Create a sparse matrix for our users and mcats containing number of purchases\n",
    "data_sparse_new = sparse.csr_matrix((purchases, (rows, cols)), shape=(len(users), len(mcats)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    User_ID                      MCAT  Number of Leads Purchased\n",
      "0         1       Android POS Machine                        121\n",
      "1         2                   Scanner                          3\n",
      "2         3  Electronic Cash Register                          0\n",
      "3         4            Cash Registers                         56\n",
      "4         1      Spot Billing Machine                          4\n",
      "5         1         Bluetooth Printer                          1\n",
      "6         2            Coaxial Cables                         76\n",
      "7         3            BNC Connectors                         23\n",
      "8         3             AC DC Adapter                         10\n",
      "9         3       Power Over Ethernet                         10\n",
      "10        4          LED Power Supply                          5\n"
     ]
    }
   ],
   "source": [
    "D_sample = pd.read_excel('Sample_head.xlsx')\n",
    "print(D_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    user_id        mcat_recommended     score\n",
      "10      460          Coaxial Cables  0.888891\n",
      "11      460          BNC Connectors  0.900179\n",
      "12      460           AC DC Adapter  0.909324\n",
      "13      460     Power Over Ethernet  0.920662\n",
      "14      460        LED Power Supply  0.920834\n",
      "15      460             Tower Light  0.928179\n",
      "16      460          Male Connector  0.940248\n",
      "17      460           Power Adapter  0.878361\n",
      "18      460              POE Switch  0.959206\n",
      "19      460  Thermal Imaging Camera  0.982029\n",
      "20     1977            Food Warmers  1.024220\n",
      "21     1977   Decorative Table Lamp  0.997653\n",
      "22     1977              Brass Lamp  0.988891\n",
      "23     1977             Lamp Shades  0.987453\n",
      "24     1977        Decorative Lamps  0.946331\n",
      "25     1977          Fancy Lanterns  0.943662\n"
     ]
    }
   ],
   "source": [
    "D2_sample = pd.read_excel('Sample_head.xlsx',sheet_name='Sheet2')\n",
    "print(D2_sample.tail(16))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of Users : 473\n",
      "Length of Mcats : 266\n",
      "Length of purchase : 14532\n",
      "Dimension of the Matrix : (473, 266)\n"
     ]
    }
   ],
   "source": [
    "print(\"Length of Users :\",len(users))\n",
    "print(\"Length of Mcats :\",len(mcats))\n",
    "print(\"Length of purchase :\",len(purchases))\n",
    "\n",
    "print(\"Dimension of the Matrix :\",data_sparse_new.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>purchase_cnt</th>\n",
       "      <th>user_id</th>\n",
       "      <th>mcat_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>11</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>11</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>7</td>\n",
       "      <td>11</td>\n",
       "      <td>92</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   purchase_cnt  user_id  mcat_id\n",
       "0             0        3       27\n",
       "1             1       11       92\n",
       "2             4       11       92\n",
       "3             7       11       92\n",
       "4             0       11       18"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    " \n",
    "\"\"\" \n",
    "Implementation of Alternating Least Squares with implicit data. We iteratively\n",
    "    compute the user (x_u) and item (y_i) vectors using the following formulas:\n",
    " \n",
    "    x_u = ((Y.T*Y + Y.T*(Cu - I) * Y) + lambda*I)^-1 * (X.T * Cu * p(u))\n",
    "    y_i = ((X.T*X + X.T*(Ci - I) * X) + lambda*I)^-1 * (Y.T * Ci * p(i))\n",
    " \n",
    "    Args:\n",
    "        sparse_data (csr_matrix): Our sparse user-by-item matrix\n",
    " \n",
    "        alpha_val (int): The rate in which we'll increase our confidence\n",
    "        in a preference with more interactions.\n",
    " \n",
    "        iterations (int): How many times we alternate between fixing and \n",
    "        updating our user and item vectors\n",
    " \n",
    "        lambda_val (float): Regularization value\n",
    " \n",
    "        features (int): How many latent features we want to compute.\n",
    "    \n",
    "    Returns:     \n",
    "        X (csr_matrix): user vectors of size users-by-features\n",
    "        \n",
    "        Y (csr_matrix): item vectors of size items-by-features\n",
    "     \"\"\"\n",
    "\n",
    "   \n",
    "\n",
    "def implicit_als(sparse_data, alpha_val=40, iterations=10, lambda_val=0.1, features=10):\n",
    "     # Calculate the foncidence for each value in our data\n",
    "    confidence = sparse_data * alpha_val\n",
    "    \n",
    "    # Get the size of user rows and item columns\n",
    "    user_size, item_size = sparse_data.shape\n",
    "    \n",
    "    # We create the user vectors X of size users-by-features, the item vectors\n",
    "    # Y of size items-by-features and randomly assign the values.\n",
    "    X = sparse.csr_matrix(np.random.normal(size = (user_size, features)))\n",
    "    Y = sparse.csr_matrix(np.random.normal(size = (item_size, features)))\n",
    "    \n",
    "    #Precompute I and lambda * I\n",
    "    X_I = sparse.eye(user_size)\n",
    "    Y_I = sparse.eye(item_size)\n",
    "    \n",
    "    I = sparse.eye(features)\n",
    "    lI = lambda_val * I\n",
    "    \n",
    "    for i in range(iterations):\n",
    "        print('iteration %d of %d' % (i+1, iterations))\n",
    "        \n",
    "        # Precompute Y-transpose-Y and X-transpose-X\n",
    "        yTy = Y.T.dot(Y)\n",
    "        xTx = X.T.dot(X)\n",
    "\n",
    "        # Loop through all users\n",
    "        for u in range(user_size):\n",
    "\n",
    "            # Get the user row.\n",
    "            u_row = confidence[u,:].toarray() \n",
    "\n",
    "            # Calculate the binary preference p(u)\n",
    "            p_u = u_row.copy()\n",
    "            p_u[p_u != 0] = 1.0\n",
    "\n",
    "            # Calculate Cu and Cu - I\n",
    "            CuI = sparse.diags(u_row, [0])\n",
    "            Cu = CuI + Y_I\n",
    "\n",
    "            # Put it all together and compute the final formula\n",
    "            yT_CuI_y = Y.T.dot(CuI).dot(Y)\n",
    "            yT_Cu_pu = Y.T.dot(Cu).dot(p_u.T)\n",
    "            X[u] = spsolve(yTy + yT_CuI_y + lI, yT_Cu_pu)\n",
    "\n",
    "    \n",
    "        for i in range(item_size):\n",
    "\n",
    "            # Get the item column and transpose it.\n",
    "            i_row = confidence[:,i].T.toarray()\n",
    "\n",
    "            # Calculate the binary preference p(i)\n",
    "            p_i = i_row.copy()\n",
    "            p_i[p_i != 0] = 1.0\n",
    "\n",
    "            # Calculate Ci and Ci - I\n",
    "            CiI = sparse.diags(i_row, [0])\n",
    "            Ci = CiI + X_I\n",
    "\n",
    "            # Put it all together and compute the final formula\n",
    "            xT_CiI_x = X.T.dot(CiI).dot(X)\n",
    "            xT_Ci_pi = X.T.dot(Ci).dot(p_i.T)\n",
    "            Y[i] = spsolve(xTx + xT_CiI_x + lI, xT_Ci_pi)\n",
    "\n",
    "    return X, Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#raw2['user'] = raw2['user'].astype(\"category\").cat.codes\n",
    "iterations = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 1 of 5\n",
      "iteration 2 of 5\n",
      "iteration 3 of 5\n",
      "iteration 4 of 5\n",
      "iteration 5 of 5\n"
     ]
    }
   ],
   "source": [
    "user_vecs1, item_vecs1 = implicit_als(data_sparse_new, iterations=5, features=20, alpha_val=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    27\n",
       "1    92\n",
       "2    92\n",
       "3    92\n",
       "4    18\n",
       "Name: mcat_id, dtype: int16"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.mcat_id.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     mcat      score\n",
      "0   25557  14.590245\n",
      "1   95670   4.530865\n",
      "2    5329   3.488501\n",
      "3   95673   3.421736\n",
      "4    9046   3.378059\n",
      "5   20046   3.276832\n",
      "6   95566   3.215481\n",
      "7  180792   3.054726\n",
      "8   95581   2.658631\n",
      "9   34776   2.655455\n"
     ]
    }
   ],
   "source": [
    "#------------------------------\n",
    "# FIND SIMILAR MCATS\n",
    "#------------------------------\n",
    "\n",
    "# Let's find similar mcats to mcat_id 92. \n",
    "# Note that this ID might be different for you if you're using\n",
    "# the full dataset or if you've sliced it somehow. \n",
    "mcat_id = 92\n",
    "\n",
    "# Get the item row for Jay-Z\n",
    "item_vec = item_vecs1[mcat_id].T\n",
    "\n",
    "# Calculate the similarity score between mcats=1 and other mcats\n",
    "# and select the top 10 most similar.\n",
    "scores = item_vecs1.dot(item_vec).toarray().reshape(1,-1)[0]\n",
    "top_10 = np.argsort(scores)[::-1][:10]\n",
    "\n",
    "mcats = []\n",
    "mcats_scores = []\n",
    "\n",
    "# Get and print the actual mcat names and scores\n",
    "for idx in top_10:\n",
    "    mcats.append(item_lookup.mcat.loc[item_lookup.mcat_id == str(idx)].iloc[0])\n",
    "    mcats_scores.append(scores[idx])\n",
    "\n",
    "similar = pd.DataFrame({'mcat': mcats, 'score': mcats_scores})\n",
    "\n",
    "print(similar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   mcat_id    mcat\n",
      "1       92   25557\n",
      "19     224  142414\n",
      "20       1     210\n",
      "21       6    2906\n",
      "44     114   34776\n",
      "     mcat     score\n",
      "0    3640  1.000000\n",
      "1   95596  0.950159\n",
      "2   95569  0.910121\n",
      "3   16636  0.885380\n",
      "4     213  0.848514\n",
      "5  132912  0.794544\n",
      "6   16039  0.794363\n",
      "7   39409  0.790214\n",
      "8   95619  0.773000\n",
      "9   13613  0.763179\n"
     ]
    }
   ],
   "source": [
    "# Let's say we want to recommend artists for user with ID 2023\n",
    "user_id = 11\n",
    "\n",
    "#------------------------------\n",
    "# GET ITEMS CONSUMED BY USER\n",
    "#------------------------------\n",
    "\n",
    "# Let's print out what the user has listened to\n",
    "consumed_idx = data_sparse_new[user_id,:].nonzero()[1].astype(str)\n",
    "consumed_items = item_lookup.loc[item_lookup.mcat_id.isin(consumed_idx)]\n",
    "print(consumed_items)\n",
    "\n",
    "\n",
    "#------------------------------\n",
    "# CREATE USER RECOMMENDATIONS\n",
    "#------------------------------\n",
    "\n",
    "def recommend(user_id, data_sparse_new, user_vecs1, item_vecs1, item_lookup, num_items=10):\n",
    "    \"\"\"Recommend items for a given user given a trained model\n",
    "    \n",
    "    Args:\n",
    "        user_id (int): The id of the user we want to create recommendations for.\n",
    "        \n",
    "        data_sparse (csr_matrix): Our original training data.\n",
    "        \n",
    "        user_vecs (csr_matrix): The trained user x features vectors\n",
    "        \n",
    "        item_vecs (csr_matrix): The trained item x features vectors\n",
    "        \n",
    "        item_lookup (pandas.DataFrame): Used to map artist ids to artist names\n",
    "        \n",
    "        num_items (int): How many recommendations we want to return:\n",
    "        \n",
    "    Returns:\n",
    "        recommendations (pandas.DataFrame): DataFrame with num_items artist names and scores\n",
    "    \n",
    "    \"\"\"\n",
    "  \n",
    "    # Get all interactions by the user\n",
    "    user_interactions = data_sparse_new[user_id,:].toarray()\n",
    "\n",
    "    # We don't want to recommend items the user has consumed. So let's\n",
    "    # set them all to 0 and the unknowns to 1.\n",
    "    user_interactions = user_interactions.reshape(-1) + 1 #Reshape to turn into 1D array\n",
    "    user_interactions[user_interactions > 1] = 0\n",
    "\n",
    "    # This is where we calculate the recommendation by taking the \n",
    "    # dot-product of the user vectors with the item vectors.\n",
    "    rec_vector = user_vecs1[user_id,:].dot(item_vecs1.T).toarray()\n",
    "\n",
    "    # Let's scale our scores between 0 and 1 to make it all easier to interpret.\n",
    "    min_max = MinMaxScaler()\n",
    "    rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1,1))[:,0]\n",
    "    recommend_vector = user_interactions*rec_vector_scaled\n",
    "   \n",
    "    # Get all the artist indices in order of recommendations (descending) and\n",
    "    # select only the top \"num_items\" items. \n",
    "    item_idx = np.argsort(recommend_vector)[::-1][:num_items]\n",
    "\n",
    "    mcats = []\n",
    "    scores = []\n",
    "\n",
    "    # Loop through our recommended artist indicies and look up the actial artist name\n",
    "    for idx in item_idx:\n",
    "        mcats.append(item_lookup.mcat.loc[item_lookup.mcat_id == str(idx)].iloc[0])\n",
    "        scores.append(recommend_vector[idx])\n",
    "\n",
    "    # Create a new dataframe with recommended artist names and scores\n",
    "    recommendations = pd.DataFrame({'mcat': mcats, 'score': scores})\n",
    "    \n",
    "    return recommendations\n",
    "\n",
    "# Let's generate and print our recommendations\n",
    "recommendations = recommend(user_id, data_sparse_new, user_vecs1, item_vecs1, item_lookup)\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intel MKL BLAS detected. Its highly recommend to set the environment variable 'export MKL_NUM_THREADS=1' to disable its internal multithreading\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 20.0/20 [00:00<00:00, 76.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "25557\n",
      "9046\n",
      "11347\n",
      "34776\n",
      "95670\n",
      "145249\n",
      "74236\n",
      "101712\n",
      "3722\n",
      "184191\n",
      "    mcat     score\n",
      "0  53759  0.825571\n",
      "1  95670  0.789236\n",
      "2  11845  0.747133\n",
      "3    212  0.726179\n",
      "4  10057  0.709587\n",
      "5   9046  0.696731\n",
      "6   5329  0.676408\n",
      "7  16636  0.674436\n",
      "8   3640  0.642622\n",
      "9  68632  0.635805\n"
     ]
    }
   ],
   "source": [
    "#more faster approach\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import implicit # The Cython library\n",
    "\n",
    "# Load the data like we did before\n",
    "raw_data = pd.read_csv('subcat_378.csv')\n",
    "raw_data = raw_data.loc[:,['GLUSR','MAPPED_MCAT','PURCHASE_CNT']]\n",
    "#raw_data = raw_data.drop(raw_data.columns[1], axis=1)\n",
    "raw_data.columns = ['user', 'mcat', 'purchase_cnt']\n",
    "\n",
    "# Drop NaN columns\n",
    "data = raw_data.dropna()\n",
    "data = data.copy()\n",
    "\n",
    "# Create a numeric user_id and artist_id column\n",
    "data['user'] = data['user'].astype(\"category\")\n",
    "data['mcat'] = data['mcat'].astype(\"category\")\n",
    "data['user_id'] = data['user'].cat.codes\n",
    "data['mcat_id'] = data['mcat'].cat.codes\n",
    "\n",
    "# The implicit library expects data as a item-user matrix so we\n",
    "# create two matricies, one for fitting the model (item-user) \n",
    "# and one for recommendations (user-item)\n",
    "sparse_item_user = sparse.csr_matrix((data['purchase_cnt'].astype(float), (data['mcat_id'], data['user_id'])))\n",
    "sparse_user_item = sparse.csr_matrix((data['purchase_cnt'].astype(float), (data['user_id'], data['mcat_id'])))\n",
    "\n",
    "# Initialize the als model and fit it using the sparse item-user matrix\n",
    "model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=20)\n",
    "\n",
    "# Calculate the confidence by multiplying it by our alpha value.\n",
    "alpha_val = 15\n",
    "data_conf = (sparse_item_user * alpha_val).astype('double')\n",
    "\n",
    "# Fit the model\n",
    "model.fit(data_conf)\n",
    "\n",
    "\n",
    "#---------------------\n",
    "# FIND SIMILAR ITEMS\n",
    "#---------------------\n",
    "\n",
    "# Find the 10 most similar to mcat 92\n",
    "mcat_id = 92 \n",
    "n_similar = 10\n",
    "\n",
    "# Get the user and item vectors from our trained model\n",
    "user_vecs = model.user_factors\n",
    "item_vecs = model.item_factors\n",
    "\n",
    "# Calculate the vector norms\n",
    "item_norms = np.sqrt((item_vecs * item_vecs).sum(axis=1))\n",
    "\n",
    "# Calculate the similarity score, grab the top N items and\n",
    "# create a list of item-score tuples of most similar artists\n",
    "scores = item_vecs.dot(item_vecs[mcat_id]) / item_norms\n",
    "top_idx = np.argpartition(scores, -n_similar)[-n_similar:]\n",
    "similar = sorted(zip(top_idx, scores[top_idx] / item_norms[mcat_id]), key=lambda x: -x[1])\n",
    "\n",
    "# Print the names of our most similar artists\n",
    "for item in similar:\n",
    "    idx, score = item\n",
    "    print(data.mcat.loc[data.mcat_id == idx].iloc[0])\n",
    "\n",
    "\n",
    "#------------------------------\n",
    "# CREATE USER RECOMMENDATIONS\n",
    "#------------------------------\n",
    "\n",
    "def recommend(user_id, sparse_user_item, user_vecs, item_vecs, num_items=10):\n",
    "    \"\"\"The same recommendation function we used before\"\"\"\n",
    "\n",
    "    user_interactions = sparse_user_item[user_id,:].toarray()\n",
    "\n",
    "    user_interactions = user_interactions.reshape(-1) + 1\n",
    "    user_interactions[user_interactions > 1] = 0\n",
    "\n",
    "    rec_vector = user_vecs[user_id,:].dot(item_vecs.T).toarray()\n",
    "\n",
    "    min_max = MinMaxScaler()\n",
    "    rec_vector_scaled = min_max.fit_transform(rec_vector.reshape(-1,1))[:,0]\n",
    "    recommend_vector = user_interactions * rec_vector_scaled\n",
    "\n",
    "    item_idx = np.argsort(recommend_vector)[::-1][:num_items]\n",
    "\n",
    "    mcats = []\n",
    "    scores = []\n",
    "\n",
    "    for idx in item_idx:\n",
    "        mcats.append(data.mcat.loc[data.mcat_id == idx].iloc[0])\n",
    "        scores.append(recommend_vector[idx])\n",
    "\n",
    "    recommendations = pd.DataFrame({'mcat': mcats, 'score': scores})\n",
    "\n",
    "    return recommendations\n",
    "\n",
    "# Get the trained user and item vectors. We convert them to \n",
    "# csr matrices to work with our previous recommend function.\n",
    "user_vecs = sparse.csr_matrix(model.user_factors)\n",
    "item_vecs = sparse.csr_matrix(model.item_factors)\n",
    "\n",
    "# Create recommendations for user with id 2025\n",
    "user_id = 11\n",
    "\n",
    "recommendations = recommend(user_id, sparse_user_item, user_vecs, item_vecs)\n",
    "\n",
    "print(recommendations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('cat_13.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Unique Seller: 4851\n",
      "Total Unique MCAT count : 4066\n"
     ]
    }
   ],
   "source": [
    "print('Total Unique Seller:',len(raw_data.GLUSR.unique()))\n",
    "print('Total Unique MCAT count :', len(raw_data.MAPPED_MCAT.unique()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 20.0/20 [00:01<00:00, 17.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(48510, 3)\n"
     ]
    }
   ],
   "source": [
    "# Using impliciti modeling\n",
    "\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse.linalg import spsolve\n",
    "import random\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "import implicit\n",
    "\n",
    "# Load the data like we did before\n",
    "#raw_data = pd.read_csv('cat_13.csv')\n",
    "raw_data = raw_data.loc[:,['GLUSR','MAPPED_MCAT','PURCHASE_CNT']]\n",
    "#raw_data = raw_data.drop(raw_data.columns[1], axis=1)\n",
    "raw_data.columns = ['user', 'mcat', 'purchase_cnt']\n",
    "\n",
    "# Drop NaN columns\n",
    "data = raw_data.dropna()\n",
    "data = data.copy()\n",
    "\n",
    "# Create a numeric user_id and artist_id column\n",
    "data['user'] = data['user'].astype(\"category\")\n",
    "data['mcat'] = data['mcat'].astype(\"category\")\n",
    "data['user_id'] = data['user'].cat.codes\n",
    "data['mcat_id'] = data['mcat'].cat.codes\n",
    "\n",
    "# The implicit library expects data as a item-user matrix so we\n",
    "# create two matricies, one for fitting the model (item-user) \n",
    "# and one for recommendations (user-item)\n",
    "sparse_item_user = sparse.csr_matrix((data['purchase_cnt'].astype(float), (data['mcat_id'], data['user_id'])))\n",
    "sparse_user_item = sparse.csr_matrix((data['purchase_cnt'].astype(float), (data['user_id'], data['mcat_id'])))\n",
    "\n",
    "# Initialize the als model and fit it using the sparse item-user matrix\n",
    "model = implicit.als.AlternatingLeastSquares(factors=20, regularization=0.1, iterations=20)\n",
    "\n",
    "# Calculate the confidence by multiplying it by our alpha value.\n",
    "alpha_val = 15\n",
    "data_conf = (sparse_item_user * alpha_val).astype('double')\n",
    "\n",
    "#Fit the model\n",
    "model.fit(data_conf)\n",
    "\n",
    "\n",
    "#---------------------\n",
    "# FIND SIMILAR ITEMS\n",
    "#---------------------\n",
    "\n",
    "Ids = list(data.user_id.unique())\n",
    "R1 = pd.DataFrame(columns=['mcat_id','score','user_id'])\n",
    "R1\n",
    "\n",
    "\n",
    "for i in range(len(Ids)):\n",
    "    user_id = Ids[i]\n",
    "    \n",
    "    recommended = model.recommend(user_id, sparse_user_item)\n",
    "\n",
    "    mcats = []\n",
    "    scores = []\n",
    "    users = []\n",
    "\n",
    "# Get artist names from ids\n",
    "    for item in recommended:\n",
    "        idx, score = item\n",
    "        mcats.append(data.mcat.loc[data.mcat_id == idx].iloc[0])\n",
    "        #users.append(data.user.loc[data.user_id== idx].iloc[0])\n",
    "        scores.append(score)\n",
    "\n",
    "    # Create a dataframe of mcat names and scores\n",
    "    recommendations = pd.DataFrame({'mcat_id': mcats, 'score': scores})\n",
    "    recommendations['user_id'] = data.user.loc[data.user_id== Ids[i]].iloc[0]\n",
    "\n",
    "#print(recommendations)\n",
    "    \n",
    "    R1 = R1.append(recommendations)\n",
    "        \n",
    "        \n",
    "print(R1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = R1[['user_id','mcat_id','score']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2 = R2.sort_values(by='user_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "R2.to_excel('user_to_mcat_recommen_ALS_grp_id13.xlsx',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(len(Ids)):\n",
    "    user_id = Ids[i]\n",
    "    \n",
    "    recommended = model.recommend(user_id, sparse_user_item)\n",
    "\n",
    "    mcats = []\n",
    "    scores = []\n",
    "    users = []\n",
    "\n",
    "# Get artist names from ids\n",
    "    for item in recommended:\n",
    "        idx, score = item\n",
    "        mcats.append(data.mcat.loc[data.mcat_id == idx].iloc[0])\n",
    "        #users.append(data.user.loc[data.user_id== idx].iloc[0])\n",
    "        scores.append(score)\n",
    "\n",
    "    # Create a dataframe of artist names and scores\n",
    "    recommendations = pd.DataFrame({'mcat_id': mcats, 'score': scores})\n",
    "    recommendations['user_id'] = data.user.loc[data.user_id== Ids[i]].iloc[0]\n",
    "\n",
    "#print(recommendations)\n",
    "    \n",
    "    R1 = R1.append(recommendations)\n",
    "        \n",
    "        \n",
    "print(R1.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "implicit.als.AlternatingLeastSquares"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mcat_id</th>\n",
       "      <th>score</th>\n",
       "      <th>user_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>15862</td>\n",
       "      <td>1.317767</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>98602</td>\n",
       "      <td>0.740766</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>23434</td>\n",
       "      <td>0.771373</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>23435</td>\n",
       "      <td>0.922006</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>33887</td>\n",
       "      <td>0.725858</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>97</td>\n",
       "      <td>0.988622</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>25564</td>\n",
       "      <td>1.027371</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10281</td>\n",
       "      <td>1.030788</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>16423</td>\n",
       "      <td>0.971073</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>6241</td>\n",
       "      <td>1.166350</td>\n",
       "      <td>1977</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  mcat_id     score user_id\n",
       "0   15862  1.317767    1977\n",
       "8   98602  0.740766    1977\n",
       "7   23434  0.771373    1977\n",
       "6   23435  0.922006    1977\n",
       "9   33887  0.725858    1977\n",
       "4      97  0.988622    1977\n",
       "3   25564  1.027371    1977\n",
       "2   10281  1.030788    1977\n",
       "5   16423  0.971073    1977\n",
       "1    6241  1.166350    1977"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "R1.sort_values(by='user_id').head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Saving Model to the file and loading the same for further use:\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save model to the drive\n",
    "pickle.dump(model,open('c:/Users/imart/Documents/Collaborative/Model_ALS_grp_13.pkl','wb'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
